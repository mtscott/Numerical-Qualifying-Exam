\documentclass[a4paper,12pt]{article} % This defines the style of your paper



\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 



% The following two packages - multirow and booktabs - are needed to create nice looking tables.

\usepackage{multirow} % Multirow is for tables with multiple rows within one cell.

\usepackage{booktabs} % For even nicer tables.

\usepackage{setspace}

\usepackage{url}

\setlength{\parindent}{0in}

\usepackage{float}

\usepackage{amsmath,amsthm,amssymb,amsfonts, enumitem, fancyhdr, color, comment, graphicx, environ, textcomp, mathrsfs, physics}

\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbZ}{\mathbb{Z}}
\newcommand{\bbQ}{\mathbb{Q}}

\newtheorem{theorem}{Theorem}



\pagestyle{fancy}



\fancyhf{} % This makes sure we do not have other information in our header or footer.



\lhead{\footnotesize Qualifying Exam: Numerical Analysis } % Class: Assignment e.g. Abstract Algebra: Gradescope Practice



%\rhead works just like \lhead (you can also use \chead)

\rhead{\footnotesize Scott (mscott04)} 

\cfoot{\footnotesize \thepage} 





\begin{document}

\thispagestyle{empty} % This command disables the header on the first page. 



\begin{tabular}{p{15.5cm}} % This is a simple tabular environment to align your text nicely 

{\large \bf \sc{Mathematics Qualifying Exam:} Study Guide  } \\ %Course Number: Course Name

Tufts University \\ Fall 2021 - Spring 2022\\ Mitchell Scott (mscott04)  \\ %Professor's name

\hline

\\

\end{tabular} % Our tabular environment ends here.

\section{Numerical Analysis/ Linear Algebra}
\subsection{Systems of Equations}
\subsubsection{Linear Systems of Equations}
\paragraph{Gaussian elimination}
\paragraph{LU decompositions for full and sparse matrices}
\paragraph{Cholesky for full and sparse matrices}
If $\textbf{A}$ is SPD, then \textbf{LU} could become $\textbf{U} = \textbf{L}^T$.
\paragraph{Operation Counts}
\paragraph{Stability of Linear Systems (Condition Number)}
\paragraph{Stability of Gaussian elimination}
\paragraph{Basic Iterative Methods}
The basic goal of iterative methods is to solve $A\vec{x} = \vec{b}$, but by splitting of $A$:
\begin{align}
	A &= M - N\\
	A\vec{x} &= M\vec{x} - N\vec{x}\\
	M\vec{x} &= \vec{b} + N\vec{x}
\end{align}
But this leads to the questions, what is $M$? Well matrix $M$ is typically chosen so that the linear system $M\vec{z} = \vec{f}$ is ``easily solvable'' for any $\vec{f}$ either because it becomes a special type of matrix like $M$ might be diagonal, triangular, or triangular. It is also important to note that $M$ must not be singular, because then it isn't invertable, which is crucial in solving this problem. 

This means that we start with an initial guess and slowly refine it to get closer and closer to the actual answer. We set up the iterative system
\begin{align}
	Mx_{(k+1)} &= b + Nx_{(k)}
\end{align}
This means we can take the general formula $x_{k+1} = Gx_k + c$, where $G = M^{-1}N$ and $c = M^{-1}b$.
This has the Jacobian matrix that is 
\begin{align}
	G(x) = M^{-1}N
\end{align}
and to see if the iteration scheme is convergent if the spectral radius (recall that is the largest in magnitiude eigenvalue)
\begin{align}
	\rho(G) &= \rho(M^{-1}N)\\
	&< 1
\end{align}
The smaller $\rho(G)$ is, the faster we see convergence. 

You see though, we have a choice on what $\textbf{M}$ and $\textbf{N}$ are so that we get $\rho(\textbf{M}^{-1}\textbf{N})$ to be as small as possible. As with all things we care about, there is a trade off between the cost of solving the linear system with $\textbf{M}$ and how many iterations we care to take. The example is if $\textbf{M} = \textbf{A}$, then when we solve it, it can be done in one iteration, but that is a direct solve which can be very computational expensive.
\subparagraph{Jacobi}
Like I said before, we normally pick $M$ such that it has a special property to make it, nicely invertable and with a small norm. One way to do this is with the \underline{Jacobi Method}. This is where we have $A$
\begin{align}
	A &= D + L + U\\
	M &= D, \quad N = -(L + U)
\end{align}
This means that we get the iterative system
\begin{align}
	x^{(k+1)} &= \textbf{D}^{-1}\big( \vec{b} - (\textbf{L}+\textbf{U})x^{(k)}\big)
\end{align}
Now if we look at the overall solution components as opposed to the vectorized code, we get
\begin{align}
	x_i^{(k+1)} &= \frac{b_i - \sum_{j\neq i} a_{ij}x_j^{(k)}}{a_{ii}}, \quad i = 1,2,...,n
\end{align}
	\begin{itemize}
		\item \textbf{CON:} It is easy to see that we need to double store $x$ here, because we are using all of the old values to record the new values. Or, there is no update to $x$ so we need to let it go all the way through and then we can update it for the next iteration.
		\item \textbf{CON:} Its convergence rate is rather slow 
		\item \textbf{CON:} It is not guaranteed to converge, but
		\item \textbf{PRO:} It normally converges given standard practice. (e.g. if the matrix is diagonally dominant by rows)
	\end{itemize}
\subparagraph{Gauss - Seidel}
We can forgive a lot, but a slow convergence is not something that we want to forgive. It is so slow because it doesn't take advantage of the benefit of the new information available. However, \underline{Gauss-Seidel} does this by using the new component of the solution as soon as it is completed.
In terms of matrix calculations, we see that 
\begin{align}
	\textbf{M} &= \textbf{D} + \textbf{L}\\
	\textbf{N} &= -\textbf{U}\\
	x^{(k+1)} &= \textbf{D}^{-1}\big( \vec{b} - \textbf{L}x^{(k+1)} - \textbf{U}x^{(k)}\big)\\
	&= (\textbf{D} + \textbf{L})^{-1} \big( \vec{b} - \textbf{U}x^{(k)}\big)
\end{align}
Or if we care about the specific elemental components, we could get
\begin{align}
	x_i^{(k+1)} &= \frac{b_i - \sum_{j<i} a_{ij}x_j^{(k+1)} - \sum_{j>i} a_{ij}x_j^{(k)}}{a_{ii}}, \quad i = 1,..., n
\end{align}
\begin{itemize}
	\item \textbf{PRO:} Faster Convergence than Jacobi
	\item \textbf{PRO:} Don't have to keep two versions of $\vec{x}$, as we are constantly overwriting it.
	\item \textbf{CON:} Has to do these recursively, and we can't use this on a parallel computing system.
	\item \textbf{CON:} Doesn't always converge, but
	\item \textbf{PRO:} Converges under most practical applications, and less required than Jacobi (e.g. SPD matrix).
	\item \textbf{CON:} Still kind of slow to converge. 
\end{itemize}
\subparagraph{Successive Overrelaxation method}
While Gauss-Seidel is still better than Jacobi, it is still not fast. But there is a way to make it better, and that is called \\
\underline{Successive Over-Relaxation (SOR)}. We first start each new iteration by computing where the Gauss- Seidel step would take us, $x_{GS}^{(k+1)}$, but we don't just step in that direction. Instead,
\begin{align}
	\vec{x}^{(k+1)} &= \vec{x}^{(k)} + \omega \big(\vec{x}_{GS}^{(k+1)} - \vec{x}^{(k)}\big)\\
	&= (1-\omega) \vec{x}^{(k)}  + \omega \vec{x}_{GS}^{(k+1)}\label{SOR}
\end{align}
Okay, but what does this $\omega$ do? What is the optimal value of $\omega$? Your guess is as good as mine. bestie. The whole point of $\omega$ is to speed up the convergence of the GS method, so it's goal is to reduce $\rho(\textbf{M}^{-1}\textbf{N})$. I still don't know a robust way to do this.

It is also important to note that we only care about $\omega\in(0,2)$ because outside of that range, we know that it will diverge. Look at equation $\ref{SOR}$, and I feel that this is self explanatory. We also know that when $\omega<1$ gives us \textit{under}-relaxation, and $\omega>1$ gives us \textit{over}-relaxation. When $\omega=1$, we just get Gauss- Seidel. Does it matter if we are doing under or over relaxation?

\begin{align}
	\vec{x}^{(k+1)} &= \vec{x}^{(k)} + \omega\bigg(\textbf{D}^{-1}\big(\vec{b} - \textbf{L}\vec{x}^{(k+1)} - \textbf{U}\vec{x}^{(k)}\big) - \vec{x}^{(k)} \bigg)\\
	&= \big( \textbf{D}+ \omega \textbf{L}\big)^{-1} \bigg(\big(1-\omega\big)\textbf{D} - \omega\textbf{U}\bigg)\vec{x}^{(k)} + \omega \big( \textbf{D}+ \omega \textbf{L}\big)^{-1} \vec{b}
\end{align}
This implies that the splitting of the matrix $\textbf{A}$ is
\begin{align}
	\textbf{M}&= \frac{1}{\omega} \textbf{D} + \textbf{L}, \qquad \textbf{N} = \bigg( \frac{1}{\omega} - 1\bigg) \textbf{D} - \textbf{U}
\end{align}
\begin{itemize}
	\item \textbf{PRO:} Faster than Gauss- Seidel
	\item \textbf{CON:} No way to know when it will converge, also have to now consider what $\omega$ values will work with convergence.
	\item \textbf{CON:} Still a stationary method so not as fast as it could be.
\end{itemize}
	
\paragraph{Conjugate Gradient Method}

\subsubsection{Eigenvalue Problems}
Sometimes we care about eigenvalues, but it would be too computationally expensive (especially for a large sparse matrix) to compute them accurately. The current algorithms for eigenvalue finding are $\order{n^3}$. So let's say we just want to know if our iterative method is going to converge, we want to check that the $\rho(\textbf{A})<1$, but if we need to find them exactly, it would take the same amount of time to just solve it directly. 

This is why getting rough estimates of the magnitude of eigenvalues would be helpful. There is the roughest, dirtiest way to do that which is 
\begin{align}
	|\lambda| &\leq \norm{\textbf{A}}
\end{align}
where the norm can be any induced matrix norm. 
\paragraph{Gerschgorin theorem}
But maybe we don't want the roughest, dirtiest approximation. Meet \underline{Gershgorin's Theorem}, sometimes called \textit{Gershgorin's circle theorem}. 

\begin{theorem}
	Let $\textbf{A}$ be an $n\times n$ matrix. The eigenvalues $\lambda_1, \lambda_2,\cdots, \lambda_n$ are all contained in a union of $n$ disks, where the $i^{th}$ disk is centered at $a_{ii}$ with radius $\sum_{j\neq i} |a_{ij}|$.
\end{theorem}
\begin{proof}
	Let $\lambda$ be any eigenvalue, with the corresponding normalized eigenvector $\vec{x}$, (this means that $\norm{x}_{\infty} = 1$. Now take $\vec{x}_i$ which is an entry of $\vec{x}$ such that $|x_i| = 1$, and we are guraneteed to have one because of the infinity norm condition that we set in place. 
	
	Recall $\textbf{A}\vec{x} = \lambda \vec{x}$, now we have
	\begin{align}
		(\lambda - a_{ii})x_i &= \sum_{j\neq i}a_{ij}x_j
	\end{align}
Now using the Triangle inequality, we get 
	\begin{align}
		|\lambda - a_{ii}| &\leq \sum_{j\neq i} |a_{ij}| \cdot |x_j|\\
		&\leq \sum_{j\neq i} |a_{ij}|
	\end{align}
We know that $|x_j|\leq 1$, because we have the infinity norm of 1, so everything has to be less than or equal to the infinity norm.
\end{proof}

\paragraph{Power Method} 
Sometimes we don't care about all of the eigenvalues, but maybe just one (think spectral radius). There is a way to find this eigenvector/eigenvalue called Power Iteration, which multpilies an arbitrary nonzero vector by a matrix over and over again $(\textbf{A}, \textbf{A}^2, \textbf{A}^3, \cdots)$

The algorithm for this is inout $\vec{x}_0$, an arbitrary nonzero vector. Then iterate $k$ times so that $\vec{x}_{k+1} = \textbf{A}\vec{x}_k$. However, this is assuming that $\exists! \lambda_1$ of maximum modulus with corresponding $\vec{v}_1$. If that is the case, then this method will result in a vector $\vec{x}$ that is a multiple of $\vec{v}_1$.

%% Proof of why this works and intution

This method is cool 
\begin{itemize}
	\item \textbf{PRO:} Almost always works in practice, but not guarenteed.
	\item \textbf{CON:} Fails when $\vec{x}_0$ has no component in dominant eigenvector $\vec{v}_1$. This can be reduced by picking a random starting vector, or because computational arithmetic is inexact, normally there is some rounding in that vector component which will introduce a component.
	\item \textbf{CON:}This analysis was done assuming that there is a unique largest eigenvalue, but what about $\lambda = \pm \i$? They are a complex conjugate pair.
	\item \textbf{PRO:} If we start with a real matrix and a real starting vector, we will always get a real convergent sequence.
	\item \textbf{CON:} This method can lead to overflow/underflow because we are just multiplying something over and over again, so maybe we can get rid of this risk?
\end{itemize}

In fact, we deserve better, so that is where the normalized Power iteration comes in.

Just like before, we start with $\vec{x}_0$, an arbitrary nonzero vector. We introduce a loop where $\vec{y}_k = \textbf{A}\vec{x}_{k-1}$ and then we normalized $\vec{x}_k = \vec{y}_k / \norm{\vec{y}_k}_{\infty}$. This way every iteration is normalized!

%% Add more here!

\paragraph{Inverse Power Method} 
Sometimes we don't care about the largest eigenvalue, but rather the smallest. (We love our small kings!) This can be used in graph theory, like if the graph is bipartite or not. Or maybe you want to bound the conjugacy of $\vec{x}^T\textbf{A}\vec{x}$, which is bounded below by $\lambda_{min}$.

We will use the fact that the eigenvalues of $\textbf{A}^{-1}$ are the reciprocals of $\textbf{A}$. This means that the smallest eigenvalues of $\textbf{A}$ are the largest eigenvalues of $\textbf{A}^{-1}$, and we already know a way to find the largest eigenvalues of a matrix. 

This method is called the \underline{Inverse Power Method}. The algorithm is start with an arbitrary nonzero vector. Then you want to loop over an iteration index solving $\textbf{A}\vec{y}_k = \vec{x}_{k-1}$ for $\vec{y}_k$. Then you can normalize it so that $\vec{x}_k = \vec{y}_k / \norm{\vec{y}_k}_{\infty}$. 

It is also important to know that using a shift value offers far more potential for accelerating convergence, and can be used to get more eigenvalues than just the min and max one. This is super helpful in the inverse power iteration (why?). In particular, the eigenvalue of $\textbf{A} - \sigma\\textbf{I}$ of smallest magnitude is simply $\lambda - \sigma$, where $\lambda$ is an eigenvalue of $\textbf{A}$ closest to $\sigma$.
\begin{enumerate}
	\item \textbf{PRO:} If the shift is close to an actual eigenvalue, convergence is super fast.
	\item \textbf{PRO:} Can find ANY eigenvalue you want, not just the max and min.
	\item \textbf{PRO:} Super useful in computing the eigenvector corresponding to an approximate eigenvalue that has already been obtained by some other mean. 
	\item \textbf{CON:} Still linear convergence (I mean smaller $C$ but still)
\end{enumerate}
\paragraph{Stability of Eigenvalue Problems}

\subsubsection{Nonlinear System of Equations, Optimization}
\paragraph{Newton's Method}
\paragraph{Quasi-Newton Methods}
\paragraph{Fixed Point Iteration}
\paragraph{Newton and Levenberg-Marquardt Methods for Unconstrained Optimization}

\subsection{Numerical Aproximation}
\subsubsection{Interpolation}
\paragraph{Interpolating polynomials}
\subparagraph{Lagrange Interpolating Polynomials}
\subparagraph{Hermite Interpolating Polynomials}
\paragraph{Runge phenomena}
\paragraph{Splines}
\paragraph{least squares approximation of functions and orthogonal polynomials}

\subsubsection{Integration}
\paragraph{Newton-Cotes methods}
\paragraph{Gaussian quadrature}
\paragraph{Euler-MacLaurin formula}
\paragraph{Adaptive quadrature}

\subsubsection{Differential Equations}
\paragraph{Convergence of explicit one-step methods}
\paragraph{Stiffness}
\paragraph{A- stability}
\paragraph{Impossibility of A-stable explicit Runge-Kutta methods}

\subsection{References}
\begin{enumerate}
	\item An Introduction to Numerical Analysis by K. E. Atkinson (Wiley)
	\item Unconstrained Optimization by P. E. Frandsen, K. Jonasson, H. B. Nielsen, and O. Tingleff
	\item Analysis of Numerical Methods by E. Isaacson and H. B. Keller (Wiley, Dover reprint)
	\item Finite Difference Methods for Ordinary and Partial Differential Equations by R. LeVeque (SIAM)
	\item A First Course in the Numerical Analysis of Differential Equations by A. Iserles (Cambridge University Press)
\end{enumerate}

%\section{Analysis}

%\section{Algebra}

%\section{Partial Differential Equations}

%\section{Algebraic Topology}

%\section{Geometry}

\end{document}
